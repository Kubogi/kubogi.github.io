<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Using Python to Win a JavaScript Contest (CS1101S: Game of Tones) | Hai’s Blog</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Using Python to Win a JavaScript Contest (CS1101S: Game of Tones)" />
<meta name="author" content="Hai Hoang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Solving a music problem with no musical intuition" />
<meta property="og:description" content="Solving a music problem with no musical intuition" />
<link rel="canonical" href="http://localhost:4000/2025/12/28/cs1101s-sound.html" />
<meta property="og:url" content="http://localhost:4000/2025/12/28/cs1101s-sound.html" />
<meta property="og:site_name" content="Hai’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-12-28T00:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Using Python to Win a JavaScript Contest (CS1101S: Game of Tones)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Hai Hoang"},"dateModified":"2025-12-28T00:00:00+07:00","datePublished":"2025-12-28T00:00:00+07:00","description":"Solving a music problem with no musical intuition","headline":"Using Python to Win a JavaScript Contest (CS1101S: Game of Tones)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/12/28/cs1101s-sound.html"},"url":"http://localhost:4000/2025/12/28/cs1101s-sound.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Hai&apos;s Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Hai&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">kubogi.github.io</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Using Python to Win a JavaScript Contest (CS1101S: Game of Tones)</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-12-28T00:00:00+07:00" itemprop="datePublished">Dec 28, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><em>Solving a music problem with no musical intuition</em></p>

<h2 id="1-context">1. Context</h2>

<p>CS1101S is an introductory computer science course that teaches core programming concepts using Source, a JavaScript-like language. Along the way, the course runs a few optional creative contests that let students apply these ideas outside of standard problem sets.</p>

<p>Past contests include Beautiful Runes (visual art with Source runes) and The Choreographer (programmatically generated curves). I was especially drawn to Game of Tones, the sound contest, because it felt the most programmable - instead of hand-tuning visuals, it’s about generating music through code, which made it a perfect candidate for automation.</p>

<h2 id="2-finding-the-right-song">2. Finding the right song</h2>

<h3 id="constraints">Constraints</h3>

<p>Sounds simple enough, right? Not really. There were some surprisingly tight constraints I had to consider when choosing the song:</p>

<ul>
  <li>MIDI availability (non-negotiable)
    <ul>
      <li>This is an obvious requirement, and also the biggest bottleneck of all</li>
      <li>Automating music without a MIDI file is extremely difficult (I tried)</li>
    </ul>
  </li>
  <li>Complexity
    <ul>
      <li>Multiple layers, non-trivial melody</li>
      <li>Interesting enough that hand-coding would be tedious</li>
    </ul>
  </li>
  <li>Source-friendly
    <ul>
      <li>Simple instrumentation</li>
      <li>Minimal timbre, effects, or texture</li>
      <li>Sounds that can realistically be approximated from scratch in Source</li>
    </ul>
  </li>
</ul>

<p><em>(Source’s sound module is fairly minimal. There are only a few built-in waveforms, and there’s no large library of presets like you’d find in a DAW. Anything that relies on rich instruments, effects, or sound design becomes very hard to reproduce in Source.)</em></p>

<ul>
  <li>Performance constraints
    <ul>
      <li>Reasonable note count</li>
      <li>Should not lag or freeze the runtime</li>
    </ul>
  </li>
</ul>

<p><em>(For the contest, other students vote by actually running your code on their own machines. That means performance directly affects first impressions — long wait times would almost certainly hurt votes. For context, I’ve seen submissions with 2-3 minute wait times.)</em></p>

<h3 id="searching-for-songs">Searching for songs</h3>

<p>I tried several directions initially, but most failed:</p>
<ul>
  <li>Game soundtracks
    <ul>
      <li>Often Source-friendly in terms of complexity</li>
      <li>Public MIDIs are rare or nonexistent</li>
    </ul>
  </li>
  <li>Rhythm game songs
    <ul>
      <li>Fanmade MIDIs are often available (shoutout to <a href="https://www.youtube.com/@uaaaaak5622">@Uaaaaak</a>!)</li>
      <li>Extremely dense and layered, which makes it really difficult to replicate and optimize in Source</li>
      <li>Usually include heavy effects and rich instruments</li>
    </ul>
  </li>
</ul>

<audio controls="">
  <source src="/assets/audio/2026-01-06-cs1101s-sound/raputa.mp3" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>
<p><em>Song: (From maimai) “raputa” sasakure.‌UK × TJ.hangneil</em></p>

<p><em><strong>Listening note:</strong> Extremely dense layering, rapid note changes, and the use of complex instrumentation make this impractical to reproduce or optimize in Source.</em></p>

<ul>
  <li>Mainstream pop / OP-style arrangements
    <ul>
      <li>MIDIs are not usually available, which already limits candidate songs</li>
      <li>When MIDIs are available, the arrangements tend to be structurally simple</li>
      <li>Automation offers little advantage over manual coding</li>
    </ul>
  </li>
</ul>

<audio controls="">
  <source src="/assets/audio/2026-01-06-cs1101s-sound/hitchcock.mp3" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>
<p><em>Song: Yorushika - Hitchcock</em></p>

<p><em><strong>Listening note:</strong> Structurally simple with sparse layering, meaning automation offers little advantage over hand-coding.</em></p>

<p>After filtering through a lot of mainstream pop candidates, Lagtrain (by inabakumori) stood out as one of the few that actually satisfied all the constraints.</p>
<ul>
  <li>A usable MIDI was available (<a href="https://www.youtube.com/watch?v=jVgWdFfnxDQ">Shoutout to Latency!</a>)</li>
  <li>Fairly layered, but simple enough to be implemented in Source</li>
  <li>Manageable note density and acceptable performance</li>
</ul>

<p>It wasn’t the most complex song structurally, but it crossed the point where automation had a clear advantage.</p>

<audio controls="">
  <source src="/assets/audio/2026-01-06-cs1101s-sound/lagtrain.mp3" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>
<p><em>Song: inabakumori - Lagtrain</em></p>

<p><em><strong>Listening note:</strong> Just complex enough to benefit from automation, but simple enough in terms density to run smoothly in Source.</em></p>

<p>In the end, this felt less of a musical problem and more of an engineering and design problem: choosing the right input to match the system you’re building.</p>

<h2 id="3-technical-details">3. Technical details</h2>

<h3 id="instrument-selection-and-reduction">Instrument Selection and Reduction</h3>

<p>The original MIDI file contained around 15 instrument tracks. Generating all of them in Source would have been impractical, both in terms of sound quality and performance. Many tracks were either textural, redundant, or too subtle to meaningfully contribute once translated to Source’s limited sound system.</p>

<p>To address this, I filtered the MIDI down to five core instruments, using MidiEditor to listen to and isolate tracks one by one:</p>
<ul>
  <li>Vocal</li>
  <li>Piano</li>
  <li>Ocarina</li>
  <li>Percussion</li>
  <li>Bass</li>
</ul>

<p>This preserved the identity of the song while significantly reducing complexity and runtime cost. The result doesn’t need to be perfect, it just needs to be a good balance between expressiveness and practicality.</p>

<h3 id="mapping-instruments-to-source-sounds">Mapping Instruments to Source Sounds</h3>

<p>Source’s sound module is extremely basic: a handful of waveforms, ADSR envelopes, and simple effects. No presets, no filters, and no built-in tools for tasks beyond layering and envelopes. Every instrument has to be built from scratch.</p>

<p>Most of the sound design process was iterative and empirical. I relied heavily on:</p>
<ul>
  <li>Tweaking waveforms and ADSR parameters</li>
  <li>Reading Source documentation</li>
  <li>Googling synthesis techniques</li>
  <li>Most importantly: just plain trial and error (listening and tweaking the code over and over - it took a lot of time)</li>
</ul>

<p>For melodic instruments like vocals, ocarina, and bass, I used simple waveforms (triangle/square) combined with ADSR envelopes:</p>

<ul>
  <li>Ocarina: triangle + softer ADSR</li>
  <li>Vocal: square + sharper envelope</li>
  <li>Bass: short attack + low sustain</li>
</ul>

<p>However, percussion required a different approach. Instead of pitched notes, I combined noise, basic oscillators, and aggressive envelopes to approximate drums:</p>

<ul>
  <li>Kick: sine + phase modulation</li>
  <li>Snare: tone + white noise</li>
  <li>Hi-hats: shaped white noise with different decay times</li>
</ul>

<p>Fun fact: I found that Source’s built-in cello instrument actually approximated a piano chord better than the piano itself. This led to me using layered cello chords to represent piano chords, which sounds pretty cursed, but it works..</p>

<p>With the instruments in place, the next step was turning the MIDI itself into playable Source code.</p>

<h3 id="from-midi-to-source-a-tiny-compiler-pipeline">From MIDI to Source: a tiny compiler pipeline</h3>

<p>The filtered MIDI file contained nearly 1,000 notes combined. It’s clear that manually writing <code class="language-plaintext highlighter-rouge">note(...)</code> calls was no longer viable. The MIDI file already had everything I needed — I just need to translate it into something Source could actually understand.</p>

<p>I cut off the song at 49 seconds, which is exactly right after the intro ends and the first verse starts. In theory, I could code it for the entire song, but for performance purposes, I want to keep loading time under 30 seconds.</p>

<p>Here’s a high-level overview of how MIDI files work:</p>
<ul>
  <li>Things are broken down into ticks, not time</li>
  <li>MIDI notes are spilt across messages. Each message has a type (<code class="language-plaintext highlighter-rouge">note_on</code> / <code class="language-plaintext highlighter-rouge">note_off</code>), pitch, and the timestamp (in ticks) of that message.</li>
  <li>Each track/instrument has its own independent list of messages</li>
</ul>

<p>In short, MIDI doesn’t give you notes directly, it gives you events. Which means I had to pair <code class="language-plaintext highlighter-rouge">note_on</code> and <code class="language-plaintext highlighter-rouge">note_off</code> messages to get actual notes.</p>

<p><strong>Reconstructing notes from events</strong></p>

<p>This is my general idea to turn MIDI events into real notes:</p>
<ul>
  <li>Convert tick counts into timestamps (in seconds)</li>
  <li>Keep track of “active” notes (notes that have received a <code class="language-plaintext highlighter-rouge">note_on</code> but not a <code class="language-plaintext highlighter-rouge">note_off</code> yet)</li>
  <li>A <code class="language-plaintext highlighter-rouge">note_on</code> starts a note</li>
  <li>A <code class="language-plaintext highlighter-rouge">note_off</code> ends it</li>
  <li>The duration of a note is simply the difference between the two timestamps</li>
</ul>

<p><strong>Implementing in Source</strong></p>

<p>Source offers 2 ways of combining sounds:</p>
<ul>
  <li>either simultaneously through the <code class="language-plaintext highlighter-rouge">simultaneously(list(...))</code> command</li>
  <li>or iteratively through the <code class="language-plaintext highlighter-rouge">consecutively(list(...))</code> command</li>
</ul>

<p>The simplest way would be:
For every note at timestamp t, prepend t seconds of silence, then play the note, and then finally play everything simultaneously.
This works, but has some serious issues:</p>
<ul>
  <li>Performance drops extremely fast as song size grows. (From a complexity point of view, this behaves like an O(n²) approach as the number of notes grows.)</li>
  <li>Sounds in later sections would get drowned out or noticeably quieter for unclear reasons.</li>
</ul>

<p>Hence I settled for a more efficient approach:</p>
<ul>
  <li>For each instrument, all notes are played consecutively, inserting silence only where needed</li>
  <li>Each instrument track is built independently</li>
  <li>All instrument tracks are then combined using <code class="language-plaintext highlighter-rouge">simultaneously(...)</code></li>
</ul>

<p>Even though this is a lot more complicated to implement, it scaled much better and loaded significantly faster than my previous approach. (From a complexity point of view, this behaves like an O(n) approach.)</p>

<details>
  <summary>Bonus: A short runtime analysis of the two approaches</summary>

  <div>
    <p>In the naive approach, every note scheduled at time t is implemented by:</p>

    <ul>
      <li>inserting t seconds of silence</li>
      <li>followed by the actual note</li>
      <li>then playing all notes simultaneously</li>
    </ul>

    <p>If the song contains n notes with increasing start times, this means:</p>
    <ul>
      <li>silence is repeated many times</li>
      <li>each additional note adds more silence than the previous one</li>
      <li>total silence duration grows roughly with the sum of all timestamps</li>
    </ul>

    <p>In effect, the total work performed grows quadratically with the number of notes.</p>

    <p>In contrast, the final approach builds each instrument track once, inserting silence only between consecutive notes. This ensures that total work grows linearly with the number of notes.</p>

    <figure style="text-align: center; margin: 2em 0;">
  <img src="/assets/images/2026-01-06-cs1101s-sound/2.png" alt="diagram" style="max-width: 100%; height: auto; display: block; margin: 0 auto;" />
  
  <figcaption style="font-style: italic; margin-top: 0.5em; color: #666;">
    Comparison of naive and optimized sound construction
  </figcaption>
  
</figure>

  </div>
</details>

<hr />

<h2 id="4-results">4. Results</h2>

<p>The final output is a Source program that plays the first 49 seconds of Lagtrain, reconstructed entirely from MIDI data.</p>

<video controls="" width="600">
  <source src="/assets/video/2026-01-06-cs1101s-sound/1-web.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<p>The generated code loads in around 15 seconds on my machine and plays back smoothly without noticeable lag. While it’s obviously not a perfect recreation, the melody and timing are surprisingly accurate and clearly recognizable.</p>

<p>This entry ended up winning the Game of Tones contest!</p>

<figure style="text-align: center; margin: 2em 0;">
  <img src="/assets/images/2026-01-06-cs1101s-sound/1.png" alt="yay" style="max-width: 60%; height: auto; display: block; margin: 0 auto;" />
  
  <figcaption style="font-style: italic; margin-top: 0.5em; color: #666;">
    Contest leaderboards
  </figcaption>
  
</figure>

<p>I think the result worked really well because MIDI already encodes strong structural timing, which I could map seamlessly into Source’s sound model.</p>

<p>This ended up taking around 2-3 days for me, with most of the time spent on trial and error. Despite that, I still had a lot of fun in the end, and winning the contest was the cherry on top.</p>

<h2 id="5-closing-thoughts">5. Closing thoughts</h2>

<p>Overall, this project was less about recreating Lagtrain and more about building a small pipeline to translate structured data into a constrained output format. Similar to my previous Bad Apple projects (such as playing it in a terminal), the core challenge was taking an existing data representation and reshaping it so it could survive under very different limitations.</p>

<p>Although the idea sounds simple on the surface, getting it to work smoothly required many careful engineering decisions — from filtering instruments and cutting off the song, to reconstructing notes from events and choosing an efficient playback strategy in Source. Each step involved tradeoffs between accuracy, performance, and practicality.</p>

<p>In the end, it was exactly this process of designing around constraints that made the project fun and rewarding, and turning it into a working, performant result — with a contest win on top — felt especially satisfying.</p>

  </div><a class="u-url" href="/2025/12/28/cs1101s-sound.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Hai&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Hai Hoang</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>I do code sometimes</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
